## 2.1 词汇表征  
---
用one-hot来表示词向量。但是每个词的向量内积为0，这使得一些有关联的词没有了关联性。   
所以用特征表示来表示。 这样表示的两个相关联的词，向量会很相似。
![300维特征表示](https://github.com/Duanxiaodai/Deeplearning-notes/blob/master/mynotes/5%E5%BA%8F%E5%88%97%E5%8C%96%E6%A8%A1%E5%9E%8B/img/3.png)  
### 可以使用t-sne算法来可视化高维到二维的数据  
### 词嵌入的含义是每个词都嵌入在高维空间中的一个点。例如上300维空间。
## 2.2 使用词嵌入  


### 可以使用已经训练好的词嵌入模型，然后迁移到自己小量的样本集中。  
### 人脸识别中对图片的encoding与词嵌入类似。但是人脸识别中常用encoding因为图片一般不确定，一般会涉及到海量的头像，是不确定呢。而词嵌入是有一个固定的词汇表，比如10000个词汇表，这使得每个词的特征就固定了。  


## 2.3词嵌入的特性  
计算词的相似度方式  
1. 平行四边形（sim(ew,eking - eman+ewoman)）     eking-ew = eman - ewoman    
2. 余弦相似度  

## 2.4嵌入矩阵  
嵌入矩阵包含所有单词的嵌入向量。  


## 2.5学习词嵌入  


## 2.6Word2vec  
为什么会有Word2Vec，之前对文字的编码通常使用的是one-hot，也就是一个词对应一个编号，或者是一个向量，这样一篇文章就是一个稀疏矩阵。这种方式存在两个问题：1.不同于视频音频的稠密矩阵，稀疏矩阵储存和训练就很低效；2.编码的随机性导致词语之间没有任何关联性。 
Word2Vec出现解决上面两个问题。通过embedding将词语编码映射到一个新的多维空间，在这个新的空间中，词义相近的词会有相近的位置，并且最终形成地是稠密矩阵。 
Word2Vec有两种预测模型：CBOW主要用来从原始语句推测目标词汇，skip-gram用来从目标词汇推测原始语境。

